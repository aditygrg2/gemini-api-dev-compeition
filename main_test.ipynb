{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DuringChain import DuringChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = \"\"\"\n",
    "                    [{      \n",
    "                \"phone_no\": 932489237,\n",
    "                \"name\": \"Raj Patel\",\n",
    "                \"address\": {\n",
    "                \"apartment_no\": \"223, Suryasthali Appartment\",\n",
    "                \"area_street\": \"Manavta Nagar\",\n",
    "                \"landmark\": \"Hanuman Mandir\",\n",
    "                \"town_city\": \"Bengaluru\",\n",
    "                \"state\": \"Karnataka\",\n",
    "                \"pincode\": 530068\n",
    "                },\n",
    "                \"email\": \"raja23@gmail.com\",\n",
    "                \"subscriptionStatus\": false,\n",
    "                \"previousOrders\": [\n",
    "                    {\n",
    "                \"order_id\": “20234435843-1107”,\n",
    "                \"status\": \"Delivered\",\n",
    "                \"transaction\": {\n",
    "                \"transaction_id\": \"txn123dsafasd\",\n",
    "                \"status\": \"Successful\",\n",
    "                \"payment_method\": \"Amazon Pay\",\n",
    "                \"total_amount\": 3000,\n",
    "                \"timestamp\": {\n",
    "                    \"$date\": \"2024-06-14T18:55:34.443Z\"\n",
    "                }\n",
    "                },\n",
    "                \"items\": [\n",
    "                \"product_id\": 3247387,\n",
    "                \"name\": \"HRX Oversized T-Shirt\",\n",
    "                \"description\": \"Cotton-Comfy fit Oversized T-Shirt\",\n",
    "                \"category\": \"Clothing-Men-TShirt\",\n",
    "                \"average_rating\": 3,\n",
    "                \"price\": 399,\n",
    "                \"reviews\": [\n",
    "                    \"Very Comfortable and Affordable\",\n",
    "                    \"Cheap and affordable\",\n",
    "                    \"Don't BUY AT ALLLL\"\n",
    "                ]\n",
    "                ],\n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = DuringChain(user_data=user_data, user_query=\"Hi, I received my order today, but one of the items is damaged. What should I do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<DuringChainStatus.IN_PROGRESS_GENERAL: 4>,\n",
       " \"I understand you received a damaged item. I'm sorry for the inconvenience.  Could you please tell me the order ID so I can look into this for you? \")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_instance = chain.initialize_model()\n",
    "chain.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhausted",
     "evalue": "429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m callable_(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mRpcError \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/grpc/_channel.py:1176\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1170\u001b[0m (\n\u001b[1;32m   1171\u001b[0m     state,\n\u001b[1;32m   1172\u001b[0m     call,\n\u001b[1;32m   1173\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blocking(\n\u001b[1;32m   1174\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1175\u001b[0m )\n\u001b[0;32m-> 1176\u001b[0m \u001b[39mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[39mFalse\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/grpc/_channel.py:1005\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1005\u001b[0m     \u001b[39mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.RESOURCE_EXHAUSTED\n\tdetails = \"Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.250.192.106:443 {grpc_message:\"Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\", grpc_status:8, created_time:\"2024-06-17T12:31:22.585569+05:30\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI have received a damaged package\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/ivr-llm/DuringChain.py:218\u001b[0m, in \u001b[0;36mDuringChain.send_message\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend_message\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m--> 218\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49msend_message(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    219\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate_response(response)\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/vertexai/generative_models/_generative_models.py:926\u001b[0m, in \u001b[0;36mChatSession.send_message\u001b[0;34m(self, content, generation_config, safety_settings, tools, stream)\u001b[0m\n\u001b[1;32m    919\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_message_streaming(\n\u001b[1;32m    920\u001b[0m         content\u001b[39m=\u001b[39mcontent,\n\u001b[1;32m    921\u001b[0m         generation_config\u001b[39m=\u001b[39mgeneration_config,\n\u001b[1;32m    922\u001b[0m         safety_settings\u001b[39m=\u001b[39msafety_settings,\n\u001b[1;32m    923\u001b[0m         tools\u001b[39m=\u001b[39mtools,\n\u001b[1;32m    924\u001b[0m     )\n\u001b[1;32m    925\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 926\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_message(\n\u001b[1;32m    927\u001b[0m         content\u001b[39m=\u001b[39;49mcontent,\n\u001b[1;32m    928\u001b[0m         generation_config\u001b[39m=\u001b[39;49mgeneration_config,\n\u001b[1;32m    929\u001b[0m         safety_settings\u001b[39m=\u001b[39;49msafety_settings,\n\u001b[1;32m    930\u001b[0m         tools\u001b[39m=\u001b[39;49mtools,\n\u001b[1;32m    931\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/vertexai/generative_models/_generative_models.py:1022\u001b[0m, in \u001b[0;36mChatSession._send_message\u001b[0;34m(self, content, generation_config, safety_settings, tools)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m   1021\u001b[0m     request_history \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_history \u001b[39m+\u001b[39m history_delta\n\u001b[0;32m-> 1022\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model\u001b[39m.\u001b[39;49m_generate_content(\n\u001b[1;32m   1023\u001b[0m         contents\u001b[39m=\u001b[39;49mrequest_history,\n\u001b[1;32m   1024\u001b[0m         generation_config\u001b[39m=\u001b[39;49mgeneration_config,\n\u001b[1;32m   1025\u001b[0m         safety_settings\u001b[39m=\u001b[39;49msafety_settings,\n\u001b[1;32m   1026\u001b[0m         tools\u001b[39m=\u001b[39;49mtools,\n\u001b[1;32m   1027\u001b[0m     )\n\u001b[1;32m   1028\u001b[0m     \u001b[39m# By default we're not adding incomplete interactions to history.\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_validator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/vertexai/generative_models/_generative_models.py:613\u001b[0m, in \u001b[0;36m_GenerativeModel._generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, tools, tool_config)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generates content.\u001b[39;00m\n\u001b[1;32m    589\u001b[0m \n\u001b[1;32m    590\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[39m    A single GenerationResponse object\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    606\u001b[0m request \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_request(\n\u001b[1;32m    607\u001b[0m     contents\u001b[39m=\u001b[39mcontents,\n\u001b[1;32m    608\u001b[0m     generation_config\u001b[39m=\u001b[39mgeneration_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    611\u001b[0m     tool_config\u001b[39m=\u001b[39mtool_config,\n\u001b[1;32m    612\u001b[0m )\n\u001b[0;32m--> 613\u001b[0m gapic_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prediction_client\u001b[39m.\u001b[39;49mgenerate_content(request\u001b[39m=\u001b[39;49mrequest)\n\u001b[1;32m    614\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_response(gapic_response)\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/google/cloud/aiplatform_v1beta1/services/prediction_service/client.py:2125\u001b[0m, in \u001b[0;36mPredictionServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   2122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m   2124\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m-> 2125\u001b[0m response \u001b[39m=\u001b[39m rpc(\n\u001b[1;32m   2126\u001b[0m     request,\n\u001b[1;32m   2127\u001b[0m     retry\u001b[39m=\u001b[39;49mretry,\n\u001b[1;32m   2128\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   2129\u001b[0m     metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m   2130\u001b[0m )\n\u001b[1;32m   2132\u001b[0m \u001b[39m# Done; return the response.\u001b[39;00m\n\u001b[1;32m   2133\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compression \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mcompression\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m callable_(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mRpcError \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mfrom_grpc_error(exc) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[0;31mResourceExhausted\u001b[0m: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai."
     ]
    }
   ],
   "source": [
    "chain.send_message(\"I have received a damaged package\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Use the following pieces of context (JSON) which is everything of user data to answer the question at the end.\n",
      "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "    \n",
      "                    [{      \n",
      "                \"phone_no\": 932489237,\n",
      "                \"name\": \"Raj Patel\",\n",
      "                \"address\": {\n",
      "                \"apartment_no\": \"223, Suryasthali Appartment\",\n",
      "                \"area_street\": \"Manavta Nagar\",\n",
      "                \"landmark\": \"Hanuman Mandir\",\n",
      "                \"town_city\": \"Bengaluru\",\n",
      "                \"state\": \"Karnataka\",\n",
      "                \"pincode\": 530068\n",
      "                },\n",
      "                \"email\": \"raja23@gmail.com\",\n",
      "                \"subscriptionStatus\": false,\n",
      "                \"previousOrders\": [\n",
      "                    {\n",
      "                \"order_id\": “20234435843-1107”,\n",
      "                \"status\": \"Delivered\",\n",
      "                \"transaction\": {\n",
      "                \"transaction_id\": \"txn123dsafasd\",\n",
      "                \"status\": \"Successful\",\n",
      "                \"payment_method\": \"Amazon Pay\",\n",
      "                \"total_amount\": 3000,\n",
      "                \"timestamp\": {\n",
      "                    \"$date\": \"2024-06-14T18:55:34.443Z\"\n",
      "                }\n",
      "                },\n",
      "                \"items\": [\n",
      "                \"product_id\": 3247387,\n",
      "                \"name\": \"HRX Oversized T-Shirt\",\n",
      "                \"description\": \"Cotton-Comfy fit Oversized T-Shirt\",\n",
      "                \"category\": \"Clothing-Men-TShirt\",\n",
      "                \"average_rating\": 3,\n",
      "                \"price\": 399,\n",
      "                \"reviews\": [\n",
      "                    \"Very Comfortable and Affordable\",\n",
      "                    \"Cheap and affordable\",\n",
      "                    \"Don't BUY AT ALLLL\"\n",
      "                ]\n",
      "                ],\n",
      "                \n",
      "\n",
      "    Question: Find SLDJfks\n",
      "\n",
      "    Helpful Answer:\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "import vertexai\n",
    "import os\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from vertexai.generative_models import (\n",
    "    FunctionDeclaration,\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    SafetySetting,\n",
    "    HarmCategory,\n",
    "    HarmBlockThreshold,\n",
    "    Part,\n",
    "    Tool,\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "    Use the following pieces of context (JSON) which is everything of user data to answer the question at the end.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: Find {question}\n",
    "\n",
    "    Helpful Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "model = GenerativeModel(\n",
    "    \"gemini-1.5-pro-001\",\n",
    "    generation_config=GenerationConfig(temperature=0.5),\n",
    "    safety_settings=None\n",
    ")\n",
    "\n",
    "sol = prompt.format(context = user_data, question = \"SLDJfks\")\n",
    "\n",
    "print(sol)\n",
    "\n",
    "chat = model.start_chat(response_validation = False)\n",
    "response = chat.send_message(sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I cannot find the answer to your question in the provided data. \\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.candidates[0].content.parts[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
